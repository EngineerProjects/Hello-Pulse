# Hello Pulse AI Microservice

A modular, scalable, secure Python AI Microservice for the Hello Pulse collaborative platform.

## ğŸš€ Features

- **Text Generation**: Simple LLM-based text generation with both OpenAI and Ollama support
- **RAG Generation**: Retrieval-Augmented Generation with strict access filtering
- **Web Search**: Search and enhanced generation with web search results
- **AI Agents**: Create and chat with personalized AI agents
- **Multi-tenant Architecture**: Secure isolation between organizations
- **Modular Design**: Easily swap or add new LLM providers and vector databases

## ğŸ”§ Supported Integrations

### LLM Providers
- OpenAI (GPT-3.5, GPT-4, etc.)
- Ollama (for local and open-source LLMs)

### Vector Databases
- ChromaDB
- Qdrant

### Web Search
- SerpAPI

## ğŸ› ï¸ Project Structure

```
ai-service/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ main.py                  # FastAPI application entry point
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ generate.py      # LLM generation endpoints
â”‚   â”‚   â”œâ”€â”€ rag.py           # RAG-based endpoints
â”‚   â”‚   â”œâ”€â”€ search.py        # Web search endpoints
â”‚   â”‚   â””â”€â”€ agents.py        # AI agent endpoints
â”‚   â”œâ”€â”€ dependencies.py      # API dependencies
â”‚   â””â”€â”€ middleware.py        # API middleware
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ security.py          # Authentication and authorization
â”‚   â””â”€â”€ logging.py           # Logging setup
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ requests.py          # Request models
â”‚   â””â”€â”€ responses.py         # Response models
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llm_providers/
â”‚   â”‚   â”œâ”€â”€ base.py          # Base LLM provider interface
â”‚   â”‚   â”œâ”€â”€ ollama.py        # Ollama integration
â”‚   â”‚   â””â”€â”€ openai.py        # OpenAI integration
â”‚   â”œâ”€â”€ vector_databases/
â”‚   â”‚   â”œâ”€â”€ base.py          # Base vector DB interface
â”‚   â”‚   â”œâ”€â”€ chroma.py        # ChromaDB integration
â”‚   â”‚   â””â”€â”€ qdrant.py        # Qdrant integration
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base.py          # Base agent interface
â”‚   â”‚   â”œâ”€â”€ manager.py       # Agent management
â”‚   â”‚   â””â”€â”€ personalization.py # Agent personalization
â”‚   â”œâ”€â”€ websearch/
â”‚   â”‚   â”œâ”€â”€ base.py          # Base search interface
â”‚   â”‚   â””â”€â”€ serpapi.py       # SerpAPI integration
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ retriever.py     # Document retrieval
â”‚       â””â”€â”€ generator.py     # Response generation
â””â”€â”€ utils/
    â”œâ”€â”€ filtering.py         # Access control filters
    â”œâ”€â”€ embeddings.py        # Embedding utilities
    â””â”€â”€ streaming.py         # Streaming response utilities
```

## ğŸš¦ Getting Started

### Prerequisites

- Python 3.9+
- Docker and Docker Compose (for containerized deployment)
- API keys for OpenAI and SerpAPI (optional)

### Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd hello-pulse-ai-service
   ```

2. **Create and configure environment variables:**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys and configuration
   ```

3. **Run with Docker Compose:**
   ```bash
   docker-compose up -d
   ```

4. **Or install and run locally:**
   ```bash
   pip install -r requirements.txt
   uvicorn main:app --reload
   ```

5. **Access the API documentation:**
   - Swagger UI: http://localhost:8080/docs
   - ReDoc: http://localhost:8080/redoc

## ğŸ”’ Security

This microservice is designed with security in mind:

- **JWT Authentication**: Secure communication with the Go backend
- **Strict Access Filtering**: Documents are filtered by organization, user, and visibility
- **Multi-tenancy**: Organizations can have shared or isolated vector databases

## ğŸ“š API Endpoints

### Text Generation
- `POST /api/generate`: Generate text from a prompt
- `POST /api/chat`: Chat with the AI

### RAG
- `POST /api/rag/generate`: Generate an answer using RAG
- `POST /api/rag/search`: Search for documents
- `POST /api/rag/documents`: Add documents
- `GET /api/rag/documents/{document_id}`: Get document
- `DELETE /api/rag/documents/{document_id}`: Delete document

### Web Search
- `POST /api/search`: Search the web
- `POST /api/search_and_generate`: Search and generate an answer

### Agents
- `POST /api/agents`: Create an agent
- `GET /api/agents/{agent_id}`: Get agent
- `PUT /api/agents/{agent_id}`: Update agent
- `DELETE /api/agents/{agent_id}`: Delete agent
- `POST /api/agents/{agent_id}/chat`: Chat with agent

## ğŸ”§ Configuration

The service is highly configurable through environment variables. See `.env.example` for all options.

## ğŸ“– Adding New Integrations

The modular design makes it easy to add new integrations:

### Adding a New LLM Provider
1. Create a new file in `services/llm_providers/`
2. Implement the `BaseLLMProvider` interface
3. Register the provider in `services/llm_providers/__init__.py`

### Adding a New Vector Database
1. Create a new file in `services/vector_databases/`
2. Implement the `BaseVectorDatabase` interface
3. Register the database in `services/vector_databases/__init__.py`

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.